# Hi, I'm Omar Arbi ğŸ‘‹

### AI Research â€¢ ENS Paris-Saclay (MVA) & CentraleSupÃ©lec â€¢ Generative Models â€¢ Representation Learning â€¢ HPC


## ğŸš€ About Me

I like building models that **push boundaries**, stress-test assumptions, and reveal what really drives performance. My work sits where:

* **Generative models break symmetry**,
* **Representations become interpretable**, and
* **GPU constraints force creativity**.

I enjoy turning complex ML systems into **clean, efficient, and rigorous experiments**. No fluffâ€”just models, math, and good engineering.



## ğŸ”¬ What Iâ€™m Researching

### **ğŸŒ€ Diffusion Models & Noise Geometry**

* Exploring **non-Gaussian noise** (Simplex, rank-based Gaussianization) and how noise structure shifts denoising difficulty.
* Building anomaly scoring pipelines using reconstruction spectra + latent statistics.
* Designing structured latents (FiLM conditioning, spatial Z-maps, capacity annealing).

### **ğŸ“ Representation Learning & Structure Prediction**

* Vision Transformers + GNNs for structured reasoning.
* Latent geometry analysis using **tâ€‘SNE, PPCA, KL trajectories**, and internal feature probing.


## ğŸ§  Highlight Projects

### **1. Adaptive Diffusion for Anomaly Detection**

ğŸ”¹ Swapped Gaussian noise with **Simplex noise**, Gaussianized-by-rank to preserve structure.

ğŸ”¹ Saw **significant AUROC gains** with identical architecture.

ğŸ”¹ Developed diagnostics to verify no data leakage + no artifact-induced shortcuts.

ğŸ”¹ Focus on *why* noise geometry changes anomaly separability.

### **2. GPTâ€‘2 Interpretability â€” Tuned Lens vs Logit Lens**

ğŸ”¹ Trained a **Tuned Lens** to study token-level representation flow.

ğŸ”¹ Compared KL divergence trajectories to detect prompt injection patterns.

ğŸ”¹ Tuned Lens consistently outperformed Logit Lens in decoding stability & injection detection.

### **3. PPCA at Scale (GPU, Missing Data)**

ğŸ”¹ Implemented a full EM loop for PPCA with missing values, fully vectorized for GPUs.

ğŸ”¹ Benchmarked PCA vs PPCA vs mini-batch variants on massive synthetic datasets.

### **4. Lowâ€‘VRAM LLaVA Fineâ€‘Tuning**

ğŸ”¹ Fineâ€‘tuned LLaVA with **LoRA + quantization** under strict memory budgets.

ğŸ”¹ Evaluated through LLM judges (DeepSeek R1 / MedAlpaca).

ğŸ”¹ Improved reasoning structure with deliberate prompting.

### **5. Online NMF for Time Series**

ğŸ”¹ Designed a sliding-window factorization model for financial-market signals.

ğŸ”¹ Integrated hyperparameter search to stabilize dictionary evolution.

ğŸ”¹Applied the approach to financial timeâ€‘series forecasting, supported by rigorous preprocessing and benchmarking against baseline models.



## ğŸ› ï¸ Skills

**Models:** Diffusion, VAEs, Transformers, GNNs 

**Training:** LoRA, quantization, pruning, EMA, DDP, gradient checkpointing

**Math:** ELBO, variational inference, spectral analysis, latent-variable modeling

**HPC:** Slurm, A100 GPUs, CUDA profiling, memory debugging

**Frameworks:** PyTorch, timm, MONAI, PyTorch-Geometric, vLLM, Flash-Attention



## ğŸ“Œ What I'm Focusing on Next

* Understanding how **noise geometry** reshapes generative-model learning.
* Pushing more efficient training pipelines for large models.
* Strengthening my research engineering profile for roles involving **foundation models, interpretability, and generative modeling**.



## ğŸ“« Contact

* **LinkedIn:** linkedin.com/in/omar-arbi

Thanks for stopping by! ğŸš€
