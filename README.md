# Omar Arbi

**ğŸ§  AI Research Â· ENS Paris-Saclay (MVA) & CentraleSupÃ©lec** Generative Models Â· Representation Learning  âš™ï¸

MSc in Mathematics, Vision & Learning (MVA) with a background in applied mathematics and data science.

I work on **generative models and representation learning**, with a bias toward building systems that actually run; clean experiments, reproducible pipelines, honest benchmarks.

Currently finishing my master's and looking for **Applied Scientist / Research Engineering** roles in foundation models or generative AI ğŸš€


## ğŸ”¬ Research Interests

### ğŸŒŠ Diffusion models & noise geometry

Replacing IID Gaussian noise with **structured processes** (Simplex, MatÃ©rn, rank-based Gaussianization) to control denoising difficulty and improve anomaly separability.

Building principled diagnostics to understand *why* noise geometry shifts AUROC â€” not just that it does.

---

### ğŸ§© Representation learning

Probing internal feature geometry in Transformers and GNNs via **t-SNE, PPCA, KL trajectories**.

Interested in when and how representations become interpretable across layers.


## ğŸ›  Selected Work

| Project                                                 | Core idea                                                                                  |
| ------------------------------------------------------- | ------------------------------------------------------------------------------------------ |
| ğŸ¦· **Structured-noise diffusion for anomaly detection** | Simplex/MatÃ©rn noise â†’ +11.6% detection on CBCT dental pathology; validated on brain MRI   |
| ğŸ” **GPT-2 interpretability: Tuned Lens vs Logit Lens** | KL trajectory analysis for prompt injection detection; Tuned Lens consistently more stable |
| â™Ÿ **GRPO fine-tuning on Ministral-3B**                  | 4-stage curriculum (Mate-in-1 â†’ Full Game) to stabilize sparse-reward RL for chess         |
| ğŸ§¾ **Knowledge distillation: Qwen2.5 14B â†’ 1.5B**       | TF-IDF + NMF corpus curation + LoRA distillation for Arabic summarization                  |
| ğŸ“Š **GPU-accelerated PPCA with missing data**           | Fully vectorized EM loop; benchmarked against PCA/mini-batch variants at scale             |
| ğŸ“ˆ **Online NMF for financial time series**             | Sliding-window factorization with stabilized dictionary evolution                          |



## ğŸ§° Stack

**Frameworks:** PyTorch Â· PyTorch Lightning Â· MONAI Â· Transformers Â· vLLM Â· PyG 

**Training:** LoRA/QLoRA Â· DDP Â· Slurm Â· CUDA profiling Â· gradient checkpointing

**Math:** variational inference Â· ELBO Â· spectral methods Â· optimal transport


## ğŸ¯ Now

Finishing my MVA master's.

Actively looking for research engineering or applied scientist roles focused on:

* ğŸ— Foundation models
* ğŸ¨ Generative modeling
* ğŸ” Interpretability

ğŸ“ **LinkedIn:**
[https://linkedin.com/in/omararbi](https://linkedin.com/in/omararbi)

